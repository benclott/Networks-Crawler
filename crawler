#!/usr/bin/env python3

import argparse
import socket
import ssl
from html.parser import HTMLParser
import time
import sys
import urllib.parse

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443
ALL_PATHS = {'/fakebook/': False, '/accounts/logout/': True} # (Path, Boolean), (Path, Boolean), ... || True --> Path has been seen, False --> Path has not been seen


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.sessionid = ''
        self.csrftoken = ''
        self.needCookies = False
        self.count_of_requests = 0 



    def run(self):
        
        data = ''  # Store the data of the most recent recieved message
        mysocket = self.sockInit() 
        mysocket.connect((self.server, self.port))
        while(1):
            path = ""

            '''
                #Send the first request to the server
                if count == 0:
                    path = self.getNextPath(data)#new path 
                    request = self.createHTML(path[0])
                    self.paths_checked.append(path[0])
                    count = count + 1
                
                #A message has already been recieved so use feedback information to craft next request
                else:
                    path = self.getNextPath(data) # Retrive the new path to send request to. 
                    if path[0] == "200": # Ignore for now
                        request = path[1]
                    else:
                        request = self.createHTML(path[0]) # Converts the path to the correct request format
                        self.paths_checked.append(path[0]) # Mark the path as seen
                        count = count + 1
                #data = self.receive_message(mysocket)
                #print("Response:\n%s" % data.decode('ascii'))
            '''

            request = self.getNextRequest(data) # Gets the next path to check            
            # if path == '/accounts/login/?next=/fakebook/' or path == '/accounts/login': ####### HANDLE LOGIN HERE #############
            #     print('Creating Login Request')
            #     request = self.createLogInRequest(data, path)
            #     ALL_PATHS['/accounts/login'] = True

            print("REQUEST SENT: \n", request)
            self.count_of_requests +=1
            mysocket.send(request.encode('ascii'))
            data = self.receive_message(mysocket)
            #print("RRESPONSE RECIEVED: ", data)




    '''
        Case 1; Data is empty
            - Return the root path for the first request
        Case 2: Response Code is 302
            - Return the path from the location header
        Case 3: Response Code is 200
            - Send the HTML response to the Parser to 
              check for new paths and the flag
        Case 4: 400 series response code
            - TODO: currently exits program
    '''
    def getNextRequest(self, data):
        root = "/fakebook/"
        if data == '': # first run through this will be empty as nothing has been recieved
            return self.createGet(root, self.needCookies)

        brokenData = data.split("\n")
        response_code = brokenData[0].split(' ')[1]
        header_end_index = data.find('\r\n\r\n')
        header = data[:header_end_index]
        html = data[header_end_index + 4:].strip()
        self.getSessionId(header)
        self.getCSRF(header)
        print(f'Cookies: sesh - {self.sessionid} || csrf - {self.csrftoken}')

        print(f'Currently Handling: {response_code}')
        if response_code == '302': # Server returned redirected PATH
            redirectedPath = brokenData[4].split(" ")[1].strip() # Extract the new path from the response
            return self.createGet(redirectedPath, self.needCookies) # Return path that the server redirects us to
        elif response_code == '200':
            
            self.needCookies = True
            return self.handle200(html, header) # Parses the HTML and returns the next request (could be login)
        elif response_code.startswith('4'):
            print("You SUCK!!\n\n")
            sys.exit(0)
    '''
        Parses the HTML that gets returned in the Body of 200 OK
        Returns an unseen path from the parser that we can crawl next
        If the form_method from the parser is set to post, then return
        a login post request
    '''
    def handle200(self, html, header):
        parserobj = customHTML()
        parserobj.feed(html) # Send the HTML to the parser to extract links and that stuff
        
        if parserobj.form_method == 'post': # Form requires post request, return the post for logging in
            print(f'ParserObj: {parserobj.form_method}')
            ALL_PATHS['/accounts/login'] = ALL_PATHS.get('/accounts/login', True)
            return self.createLogInRequest('/accounts/login/', parserobj, header)
        else: # Did not find a form, so get the next link
            nextPath = parserobj.getUnseen()
            print(f'[Handling 200] getUnseen() found {nextPath}')
            return self.createGet(nextPath, self.needCookies)  # Return a single unseen path to visit next 
    '''
        These next two methods extract the sessionid and CSRF Token from a response header
    '''
    def getSessionId(self, header):
        start_index = header.find("sessionid=") # Look for the start index of "sessionid="
        if start_index == -1: #sessionid not present
            return -1
        start_index += len("sessionid=") # Starting index of actual id 
        
        # Assume the session ID ends at the next semicolon or the end of the string if no semicolon is found
        end_index = header.find(";", start_index)
        if end_index == -1: # No semi colon is found
            # the session ID goes until the end of the string
            end_index = len(header)
        self.sessionid = header[start_index:end_index] # Extract and return the session ID 
    def getCSRF(self, header):
        # Look for the start index of "sessionid="
        start_index = header.find("csrftoken=")
        #print(f'Looking for CSRF. Start index: {start_index}')
        if start_index == -1:
            # If "sessionid=" is not found, return None or an appropriate value
            return -1
        
        # Add the length of "sessionid=" to the start index to find where the session ID starts
        start_index += len("sessionid=")
        
        # Assume the session ID ends at the next semicolon or the end of the string if no semicolon is found
        end_index = header.find(";", start_index)
        if end_index == -1:
            # If no semicolon is found, the session ID goes until the end of the string
            end_index = len(header)
        self.csrftoken = header[start_index:end_index] # Extract and return the session ID 

    '''
        Formats and returns the proper GET request for the given path. 
        Only include cookies if specified.
    '''
    def createGet(self, path, cookies):
        print(f'Marking {path} as seen.')
        ALL_PATHS[path] = ALL_PATHS.get(path, True)
        if not cookies:
            request = f'GET {path} HTTP/1.1\r\nHost: {self.server}:{self.port}\r\n\r\n'
        else:
            request = f'GET {path} HTTP/1.1\r\nHost: {self.server}:{self.port}\r\nCookie: csrftoken={self.csrftoken}; sessionid={self.sessionid}\r\n\r\n'

        return request
    '''
        Create the login post request. This is similar to the Create Post method that you have
        TODO: Handle difference between regular login and login with the next
        - Headers for the login currently dont have a Connection: keep-alive field
    '''   
    def createLogInRequest(self, path, parserobj, header):
        body = f'username={self.username}&password={self.password}&csrfmiddlewaretoken={parserobj.csrfmiddlewaretoken}&next={parserobj.next_specified}'
        headers = f'Content-Type: application/x-www-form-urlencoded\nContent-Length: {len(body)}\nCookie: csrftoken={self.csrftoken}; sessionid={self.sessionid}'
        request = f'POST {path} HTTP/1.1\r\nHost: {self.server}:{self.port}\r\n{headers}\r\n\r\n{body}'
        return request
    
    '''
        Create and preps the socket with ssl.
        Returns the socket object.
    '''
    def sockInit(self):
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        ssl_context = ssl.create_default_context()
        mysocket = ssl_context.wrap_socket(mysocket, server_hostname=self.server)
        return mysocket

    """
        Received the message and deals with exterior crap
    """
    def receive_message(self, sock):
            # Read HTTP headers first
        response_headers = b""
        while not response_headers.endswith(b"\r\n\r\n"):
            data = sock.recv(1)  # Read one byte at a time to get headers
            if not data:
                break  # Connection closed
            response_headers += data
        
        headers = response_headers.decode('utf-8')
        print("Headers:\n", headers)
        
        # Check if the response is chunked
        if 'transfer-encoding: chunked' in headers.lower():
            response_body = self.read_chunked_body(sock)
        else:
            # If not chunked, use Content-Length to read the response body
            content_length = self.get_content_length(headers)
            response_body = b""
            if content_length:
                while len(response_body) < content_length:
                    data = sock.recv(content_length - len(response_body))
                    if not data:
                        break  # Connection closed
                    response_body += data

        print("Body:\n", response_body.decode('utf-8'))
        return headers + "\n" + response_body.decode('utf-8')

    def get_content_length(self, headers):
        for line in headers.split('\r\n'):
            if line.lower().startswith('content-length:'):
                return int(line.split(":")[1].strip())
        return None


'''Create a custom HTML class that will identify and handle each problem'''
class customHTML(HTMLParser):
    flags = [] # If we find flags in html store them here
    flag_found = False # If current page finds a flag this becomes true
    inForm = False # True if the parser is currently inside a Form
    form_method = ''
    csrfmiddlewaretoken = ''
    next_specified = ''

    '''
        Returns an unseen path for the crawler to visit next and
        then sets the seen status to True because it has been visited
    '''
    def getUnseen(self):
        for path, seen in ALL_PATHS.items():
            if seen == False:
                seen = True # Set status to true so that we dont crawl again
                return path

    '''
       Case 1: Tag is 'a'
        - Indicates that there is a link (a path) in the data
        - Add this path to a list so that it can be explored later
       Case 2: 
        - Tag is 'h3'
        - Secret flag is in the data

    '''
    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)
        # print(f'Found start tag {tag}')
        if tag == 'a':  # Check if the tag is an anchor tag
            print(f'Found start tag {tag}')
            self.handleLinks(attrs) # Adds found link to saved paths
        elif tag == 'h3': # This is where flags are
            for attr in attrs:
                if attr[0] == 'class' and 'secret_flag' in attr[1]:
                    self.flag_found = True
                    print("I FOUND A FLAG")
        elif tag == 'form':
            self.inForm = True
            self.form_method = attrs.get('method', 'get').lower() # Default form method is GET, otherwise POST
        elif tag == 'input' and self.inForm: # We have found input tags in a form to collect data from
            self.handleForm(tag, attrs)
            
    '''
        If we are in a form. Find what inputs we need and extract the CSRFMiddlewareToken
    '''
    def handleForm(self, tag, attrs):
        dict_attrs = dict(attrs)
        print(f'[in handleForm] Attributes: {dict_attrs}')
        input_type = dict_attrs.get("type")
        input_name = dict_attrs.get("name")
        # Extract CSRF token if present
        if input_type == "hidden" and input_name == "csrfmiddlewaretoken":
            self.csrfmiddlewaretoken = dict_attrs['value']
            print(f'Found Middleware: {self.csrfmiddlewaretoken}')
        elif input_type == "hidden" and input_name == "next":
            self.next_specified = dict_attrs.get("value")

    '''
        Extracts the links from the HTML and add them to a list if they are not already present
    '''
    def handleLinks(self, attrs):
        found_path = attrs.get('href')
        if found_path and self.validURL(found_path): # make sure the found path is valid
            if found_path not in ALL_PATHS: # if the found path is not already in our saved paths
                ALL_PATHS[found_path] = False # add it
                print(f'Found New Path. All: {ALL_PATHS}')

    '''
        To be called inside the state tag
        Based on which function mpping it should do something 
    '''
    def handle_data(self, data):
        #print("Encountered some data  :", data)
        if self.flag_found == True:
            self.flag_content += data.strip()
            #flags.append(self.flag_content)
            #print(f'I ADDED THE FLAG {flag_content}')
            flag_found = False # set back to false so you can find the next one
            
    
    '''
        TODO: Not sure if this works properly
        Returns true if the URL is valid and the path should be crawled.
        Returns false otherwise
    '''
    def validURL(self, url):
        # Assume base_url is the root URL of your target website
        base_url = "https://www.3700.network"
        base_netloc = urllib.parse.urlparse(base_url).netloc

        # Fully resolve relative URLs against the base_url
        full_url = urllib.parse.urljoin(base_url, url)
        parsed = urllib.parse.urlparse(full_url)

        # Now, parsed contains the full URL. You can check its netloc against your base netloc
        return parsed.netloc == base_netloc and parsed.scheme in ["http", "https"]
            

    '''
        We have reached the end of the tag so move on somehow and then recall handleState
    '''
    def handle_endtag(self, tag):
        #print("Encountered an end tag :", tag)
        pass

    
    '''
        Method to build a post request with the provided data from class
        1. Take in all day
        2. Create a string of all necessary tags 

        The idea is that this will send a post, then we will skip the sending of a get
        After that we will read and start over

        Should return a path to a post message
    '''
    def sendPost(self):
        pass

    


    ##Send 
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()


