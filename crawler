#!/usr/bin/env python3

import argparse
import socket
import ssl
from html.parser import HTMLParser
import time
import sys
import urllib.parse

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

    paths_checked = []

    def run(self):
        count = 0 # Counts how many requests we have set
        data = ''  # Store the data of the most recent recieved message
        mysocket = self.sockInit() 
        mysocket.connect((self.server, self.port))
        while(count <= 2):
            path = ""

            # #Send the first request to the server
            # if count == 0:
            #     path = self.getNextPath(data)#new path 
            #     request = self.createHTML(path[0])
            #     self.paths_checked.append(path[0])
            #     count = count + 1
            
            # #A message has already been recieved so use feedback information to craft next request
            # else:
            #     path = self.getNextPath(data) # Retrive the new path to send request to. 
            #     if path[0] == "200": # Ignore for now
            #         request = path[1]
            #     else:
            #         request = self.createHTML(path[0]) # Converts the path to the correct request format
            #         self.paths_checked.append(path[0]) # Mark the path as seen
            #         count = count + 1
            # #data = self.receive_message(mysocket)
            # #print("Response:\n%s" % data.decode('ascii'))

            path = self.getNextPath(data) # Gets the next path to check
            if path == '/accounts/login/?next=/fakebook/':
                ####### HANDLE LOGIN HERE #############
                pass
            request = self.createHTML(path) # Converts the path to the correct request format
            self.paths_checked.append(path) # Mark the path as seen so it does not get checked again
            count = count + 1

            print("REQUEST SENT: ", path)
            mysocket.send(request.encode('ascii'))
            data = mysocket.recv(1000000).decode('ascii')
            #print("RRESPONSE RECIEVED: ", data)
            

    '''
        Create and preps the socket with ssl.
        Returns the socket object.
    '''
    def sockInit(self):
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        ssl_context = ssl.create_default_context()
        mysocket = ssl_context.wrap_socket(mysocket, server_hostname=self.server)
        return mysocket

    #Received the message and deals with exterior crap
    def receive_message(self, sock):
        
        response = "" # Empty buffer
        while(True):
            try:
                data = sock.recv(4096)
            except sock.TimeoutError:
                break     
            if(data == b'\n'): # We are at the end of the json
                break
            
            if(data == b''): # We are at the end of anything to be read
                break
            response += data.decode('ascii')
        return response

    '''
    Method to handle the response of the server
    This is where we initialize and call the html library we have created
    Should be looped through in the run and all the magic of html understanding happens here
    will create the path to send the next request and then also the message
    '''
    def getNextPath(self, data):
        root = "/fakebook/"
        if data == '': # first run through this will be empty as nothing has been recieved
            return root

        brokenData = data.split("\n") # Split the response into lines
        response_code = brokenData[0].split(' ')[1] # Extract response code from line 1
        print(f'Recieved {response_code}')
        if response_code == '302': # Server returned redirected PATH
            redirectedPath = brokenData[4].split(" ")[1].strip() # Extract the path from the response
            return redirectedPath # Return path that the server redirects us to
        elif response_code == '200':
            # Split the response into header and body parts
            header_end_index = data.find('\r\n\r\n')
            header = data[:header_end_index]
            html = data[header_end_index + 4:].strip()

            #print(html)
            parser = customHTML()
            parser.feed(html)  # Call feed method to start parsing
            return parser.getUnseen()
        
        elif response_code.startswith('4'):
            print("You SUCK!!\n\n")
            sys.exit(0)


    #Method to decide which server/host to send the website too
    def sendto(self):
        ##Check if args.s exissts
        s = args.server
        p = args.port
        if s == "proj5.3700.network" and p == 443:
            #The default has been provided so...
            return ("www.3700.network", 443)
        else:
            return (s, p)

    def createHTML(self, path):
        httptype = "HTTP/1.1"
        #request = f'GET {} {}\r\nHost: {}:{}\r\nAccept-Encoding: identity\r\n\r\n'.format(path, httptype, self.server, self.port)
        request = 'GET {} HTTP/1.1\r\nHost: {}:{}\r\n\r\n'.format(path, self.server, self.port)
        return request


'''Create a custom HTML class that will identify and handle each problem'''
class customHTML(HTMLParser):
    paths = [] # (Path, Boolean) True --> Path has been seen, False --> Path has not been seen
    flags = []
    flag_found = False

    '''
    Returns an unseen path for the crawler to visit next and
    then sets the seen status to True because it has been visited
    '''
    def getUnseen(self):
        for path in self.paths:
            if path[1] == False:
                path[1] = True
                return path[0]

    '''
    Look for start tags

    Figure out the start tag and then call the necessary next function
    will have base case for if everything is empty 
    '''
    def handle_starttag(self, tag, attrs):
        #print('Ecountered start ', tag)
        if tag == 'a':  # Check if the tag is an anchor tag
            for attr in attrs: # loop through all of the paths found
                if attr[0] == 'href':
                    paths = {path for path, seen in self.paths}
                    if attr[1] not in paths: # Path has not been encountered, add it to the list
                        #print(f'New Path {attr[1]}')
                        if self.validURL(attr[1]):
                            self.paths.append([attr[1], False])  # Append the URL to the list of URLs
                            print('Added new route. Current Paths and Status: ', self.paths)
                        else:
                            #print(f'Bad URL {attr[1]} ... Ignoring')
                            continue
                    else:
                        #print(f'Already seen {attr[1]}... Ignoring')
                        continue
                            
        if tag == 'h3':
            for attr in attrs:
                if attr[0] == 'class' and 'secret_flag' in attr[1]:
                    self.flag_found = True
                    print("I FOUND A FLAG")

    '''
    To be called inside the state tag
    Based on which function mpping it should do something 
    '''
    def handle_data(self, data):
        #print("Encountered some data  :", data)
        if self.flag_found == True:
            self.flag_content += data.strip()
            flags.append(self.flag_content)
            print(f'I ADDED THE FLAG {flag_content}')
            flag_found = False # set back to false so you can find the next one
            
    
    '''
        Returns true if the URL is valid and the path should be crawled.
        Returns false otherwise
    '''
    def validURL(self, url):
        parsed = urllib.parse.urlparse(url).netloc.strip()
        base = urllib.parse.urlparse("https://www.3700.network").netloc
        if parsed != "":
            #print(f'Comparison: {base} and {parsed}, {base == parsed}')
            return base == parsed
        else:
            return True
            

    '''
    We have reached the end of the tag so move on somehow and then recall handleState
    '''
    def handle_endtag(self, tag):
        #print("Encountered an end tag :", tag)
        pass

    
    '''
    Method to build a post request with the provided dat from class
    1. Take in all day
    2. Create a string of all necessary tags 

    The idea is that this will send a post, then we will skip the sending of a get
    After that we will read and start over

    Should return a path to a post message
    '''
    def sendPost(self):
        pass

    


    ##Send 
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()


