# Overview:

Upon execution, the web crawler initiates by establishing a connection to the Fakebook server using the provided server address and port. The crawler begins exploring from the root path `/fakebook/`, and then extracts links from the html returned. The crawler then saves these links for later and will iterate through a list of saved paths traversing each page of HTML while searching for the flags. It sends HTTP GET requests to each path and awaits the server's response. As the crawler receives responses from the server, it breaks it up into the receive message which will read the message byte by byte. The receive message then further attempts to understand if it is chunked in which case it will break it up based on size and only include the relevant parts of the data (not the ending or after the content length). Finally, it combines the headers as ready byte by byte and the chunked messages to create the data. This way of doing it ensures we do not miss any data or send a message based solely on the header when we need something from the chunked message. The HTML in the body is parsed to identify new links and extract any secret flags embedded within the HTML. If the response contains a login form, the crawler automatically submits the username, password, and csrfmiddlewaretoken to login, it then uses the sessionid and csrf token received for all subsequent requests when logged in. Throughout the crawling process, the script handles various HTTP response codes, including 200 (OK), 302 (Redirect), and client errors (e.g., 404). Error handling mechanisms ensure graceful management of errors, maintaining stability and continuity in the crawling process. To maintain authenticated interaction with the server, the crawler utilizes session cookies and CSRF tokens. These tokens are extracted from the server's response headers and included in subsequent requests and updated as necessary. As the crawler continues to explore the website, The implementation of the web crawler prioritizes efficiency and reliability, ensuring that it effectively gathers information while maintaining stability throughout the crawling process. In order to increase efficiency, we used gzip encoding to compress HTTP requests and specified `Connection: closed` as well. Overall, the execution of the program involves a systematic exploration of the Fakebook website, with the crawler navigating through paths, extracting information, and handling interactions with the server to fulfill its crawling objectives.

Diving into the workings of the HTMLParser, The customHTML class is a Python implementation extending the HTMLParser class, primarily used for parsing HTML content and extracting relevant information. Upon initialization, various attributes such as flags, flag_found, inForm, form_method, csrfmiddlewaretoken, and next_specified are set to manage the parsing process. During parsing, the handle_starttag method is invoked when encountering HTML start tags. This method routes the parsing flow based on the tag type: for anchor tags (`<a>`), it calls handleLinks to process new paths; for `<h3>` tags, it triggers handleSecrets to mark a flag as found so that handleData can extract the characters of the flag; for form (`<form>`) tags, it sets inForm and retrieves form attributes; for input tags within forms, it extracts essential input data using handleForm so that login POST requests can be sent.
The handleSecrets method examines `<h3>` tags to identify secret flags marked by the secret_flag class. Upon discovery, it sets flag_found to signal flag detection. handleForm focuses on processing input tags within forms, extracting CSRF tokens and next paths if available. The handleLinks function captures valid links found within anchor tags, storing them for potential exploration. During data parsing with handle_data, if flag_found is true, it extracts flag content and stores it. Finally, validURL checks URL validity for crawling purposes.

## Challenges:
A major challenge we ran into came when trying to login to the site. Even though we were properly extracting the middleware token, we were still unable to login. We identified the source of the issue as the cookies we sent with the log in request were incorrect. This led us to introduce the boolean flag in the crawler class needCookies. This is false on the first request to `/fakebook/` because we do not have any cookies yet. Once we receive them in the response, this flag is now set to true as these cookies should be used in the subsequent requests, including the login. The cookies are then updated with the authenticated ones after we login. Another issue popped up once we were logged in. We found that the next request to the root directory after logging in would instead be to the login form again. Sometimes it would log us out, and we were stuck in a loop of logging in and out. The solution was to plug the main paths into our ALL_PATHS tracker and mark them as seen so that we do not explore them more than needed and then get stuck in the infinite loop. We also excluded the `/accounts/logout/` path to make sure that we stay logged into the page though our code supports logging in whenever a form is encountered.
The second major issue that we ran into was that we ran into infinite loops when crawling the site. At first, we tracked seen packets in a list, locally in the HTMLParser class. The paths would be marked as seen in the getUnseen() method when they are sent to the crawler class; however, this did not cover all requests sent. To solve this issue, we made the path tracker a global variable so that it can be consistent across both classes. Now, the status of a path gets updated to be seen once the request has been created in either createGet() or createLogInRequest(), ensuring that these paths are marked as seen right before the requests for the path get sent.

Testing
To test and debug our code, we printed out the requests that we were sending and the HTTP headers and the HTML body of the response from the server to understand the interaction better.

‘Printed out HTML and headers’
‘Printed out requests that were being sent to identify contents’
‘Looked at how the requests actually wore on Firefox so we could understand how it works’
‘Tracked the pages we saw to ensure no duplicates’

###Examples###
Some major issues we ran into during testing were ‘hanging after a GET request’, ‘not getting a response after a GET request at all’, and ‘ensuring the server knew the login was sent due to a lack of cookies’. For the hanging after a GET request, we figured out this had to do with the connection and whether or not we were requesting it kept closed but then we were reading until the end of the bytes. Essentially, the combination of the receive and the keep connection closed was not compatible at first, which we determined by understanding HTTP by monitoring what happened using the webtools on Firefox. The second most relevant issue was the idea that we were just not getting a response back. This was an issue because despite print statements, we couldn’t see what was happening on the server side. We eventually delved into understanding HTTP and asked ourselves if we were sending a bad request, why weren’t we getting a 4XX error back? Eventually, we determined that our Get request might have been missing some headers, by re-writing the GET request and comparing it to the raw form of a real GET request we were able to overcome this issue.
